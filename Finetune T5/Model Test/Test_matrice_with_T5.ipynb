{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a896bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\Finetune T5\\C4 Dataset\\C4_200M.tsv-00001-of-00010\", sep='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13dc6f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(763, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f00766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d221b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xóa các dòng bắt đầu bằng \"===\"\n",
    "data.rename(columns={0: \"error\", 1: \"correct\"}, inplace=True)\n",
    "data = data[~data['correct'].str.startswith('==')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b116551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loại các dòng trong cột error có các số lượng từ >30\n",
    "data = data[data['error'].str.split().str.len() <= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c6c5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(763, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6257e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               error  \\\n",
      "1  EverySaturday we have the King Of Club 'Drawin...   \n",
      "2  Just then background of lemonade stand was ink...   \n",
      "5  Using It A2 Hosting Managed WP Discount Coupon...   \n",
      "6  The full “bar break” system is tensioned durin...   \n",
      "7  i then completed MA in creative writing, again...   \n",
      "\n",
      "                                             correct  \n",
      "1  Every Saturday we have the King Of Clubs Drawing.  \n",
      "2  Then background of the lemonade stand was inke...  \n",
      "5  How to use A2 Hosting Managed WP Discount Coup...  \n",
      "6  A full “bar break” system when loaded in tensi...  \n",
      "7  I then completed an MA in creative writing, ag...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "292372a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={0: \"error\", 1: \"correct\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2fb3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error      The bell startled her from her daydream, and s...\n",
      "correct    The bell startled her from her daydream, and s...\n",
      "Name: 339, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97bc3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                   error  \\\n",
      "1                                                                                                                                    EverySaturday we have the King Of Club 'Drawing ...   \n",
      "2        Just then background of lemonade stand was inked them Distress inks- I think they were Tumbled Glass Faded Jeans Scattered Straw Crushed Olive Mustard Seed aDd Dried Marigold.   \n",
      "5                                                                                                                                  Using It A2 Hosting Managed WP Discount Coupon codes?   \n",
      "6  The full “bar break” system is tensioned during load for for to-destruction, with the break outside for the influence of the connection, used together with BS 4449 graded 500 rebar.   \n",
      "7                                                       i then completed MA in creative writing, again in still nursing and so then decided to write a book about nurse as a profession.   \n",
      "\n",
      "                                                                                                                                                                               correct  \n",
      "1                                                                                                                                    Every Saturday we have the King Of Clubs Drawing.  \n",
      "2  Then background of the lemonade stand was inked with Distress inks - I think they were Tumbled Glass, Faded Jeans, Scattered Straw, Crushed Olive, Mustard Seed and Dried Marigold.  \n",
      "5                                                                                                                              How to use A2 Hosting Managed WP Discount Coupon Codes?  \n",
      "6                     A full “bar break” system when loaded in tension to destruction, with the break outside the influence of the connection, when used with BS 4449 grade 500 rebar.  \n",
      "7                                I then completed an MA in creative writing, again whilst still nursing, and it was then that I decided to write a book about nursing as a profession.  \n",
      "Index(['error', 'correct'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "138ec52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_sentences = [str(sentence) for sentence in data[\"error\"]]\n",
    "ref_sentences = [str(sentence) for sentence in data[\"correct\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8556e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "def decontracted(phrase): #hàm chuyển từ viết tắt thành từ đầy đủ\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"gon na\", \" going to\", phrase)\n",
    "    phrase = re.sub(r\"wan na\", \" want to\", phrase)\n",
    "    phrase = re.sub(r\"gonna\", \" going to\", phrase)\n",
    "    phrase = re.sub(r\"wanna\", \" want to\", phrase)\n",
    "\n",
    "\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def clean_text(t): #hàm bỏ dấu và ký tự unicode\n",
    "\n",
    "  t = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('ascii') \n",
    "  t = decontracted(t)\n",
    "\n",
    "  t = re.sub(r'x D', '', t)\n",
    "  t = re.sub(r': D', '', t)\n",
    "  t = re.sub(r': P', '', t)\n",
    "\n",
    "  t = re.sub(r'xD', '', t)\n",
    "  t = re.sub(r':D', '', t)\n",
    "  t = re.sub(r':P', '', t)\n",
    "\n",
    "  if '(' in t and ')' in t: #loại bỏ nội dung trong dấu ngoặc đơn => Hello (world) => Hello\n",
    "    try:\n",
    "      t = re.sub(t.split(\"(\")[-1].split(\")\")[0], '', t)\n",
    "    except:\n",
    "      pass\n",
    "    #t = re.sub(\"(\", '', t)\n",
    "    #t = re.sub(\")\", '', t)\n",
    "\n",
    "  t = re.sub(r'[^A-Za-z;!?.,\\-\\' ]+', ' ', t) #loại bỏ các emoji, số, ký tự đặc biệt\n",
    "\n",
    "#chuẩn hóa dấu câu, thêm khoảng trống trước dâu câu. \n",
    "  t = re.sub(r'\\.+',r' .',t)\n",
    "  t = re.sub(r'\\;+',r' , ',t)\n",
    "  t = re.sub(r'!+',r' !',t )\n",
    "  t = re.sub(r'\\?+',r' ?',t )\n",
    "  t = re.sub(r'\\-+',r' - ',t )\n",
    "  t = re.sub(r'\\,+',r' , ',t )\n",
    "  t = re.sub(r'\\'+',r\" ' \",t)\n",
    "  t = re.sub(' +', ' ', t)\n",
    "  t = t.lower()\n",
    "\n",
    "  return t #trả về văn bản được làm sạch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c654925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sentences = [clean_text(sentence) for sentence in ref_sentences]\n",
    "error_sentences = [clean_text(sentence) for sentence in error_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7500bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc4416c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"C:\\Users\\Admin\\Downloads\\results\\finetune_t5_latest_1_million\\checkpoint-4000\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0ed5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu đầu vào sai ngữ pháp: i want to eating banana.\n",
      "Câu sửa bởi model: i want to eat bananas .\n"
     ]
    }
   ],
   "source": [
    "input_text = \"i want to eating banana.\"\n",
    "inputs = tokenizer(input_text,return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Câu đầu vào sai ngữ pháp:\", input_text)\n",
    "print(\"Câu sửa bởi model:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c54fe73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_sentences = []\n",
    "for sentence in error_sentences[:300]:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=40)\n",
    "    if len(sentence.split()) > 40:\n",
    "        decoded = sentence\n",
    "    else:\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            preds = model.generate(**inputs, max_length=40)\n",
    "        decoded = tokenizer.decode(preds[0], skip_special_tokens=True)\n",
    "    outputs_sentences.append(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3147b294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61b37420",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sentences_formattted = []\n",
    "for sentence in outputs_sentences:\n",
    "    '''\n",
    "    sentence = sentence.replace(\",\",\" ,\")\n",
    "    sentence = sentence.replace(\".\",\" .\")\n",
    "    sentence = sentence.replace(\"!\",\" !\")\n",
    "    sentence = sentence.replace(\"?\",\" ?\")\n",
    "    sentence = sentence.replace(\"  \",\" \")\n",
    "    sentence = sentence.replace(\";\",\" ;\")\n",
    "    sentence = sentence.replace(\":\",\" :\")\n",
    "    '''\n",
    "    sentence = sentence.replace(\" -\", \"-\")\n",
    "    sentence = sentence.lower()\n",
    "    output_sentences_formattted.append(sentence)\n",
    "\n",
    "outputs_sentences = output_sentences_formattted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f00db841",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sentences = [sentence.lower() for sentence in ref_sentences]\n",
    "error_sentences = [sentence.lower() for sentence in error_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c22eb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viết câu sai vào file source.txt\n",
    "with open(r\"C:\\Users\\Admin\\Downloads\\errant_eva\\source_c4.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in error_sentences[:300]:\n",
    "        f.write(sentence + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "52207cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Admin\\Downloads\\errant_eva\\reference_c4.txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "    for sentence in ref_sentences[:300]:\n",
    "        f.write(sentence + \"\\n\")\n",
    "\n",
    "with open(r\"C:\\Users\\Admin\\Downloads\\errant_eva\\hypothesis_c4.txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "    for sentence in outputs_sentences[:300]:\n",
    "        f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c9d7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sentences: updated postoffice and coolant handling for datron mcr posts .\n",
      "Ref sentences: updated retract and coolant handling for datron mcr posts .\n",
      "Error sentences: the updated postoffice and coolant handling for datron mcr posts .\n"
     ]
    }
   ],
   "source": [
    "i = 39\n",
    "print(f\"Output sentences: {outputs_sentences[i]}\")\n",
    "print(f\"Ref sentences: {ref_sentences[i]}\")\n",
    "print(f\"Error sentences: {error_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "032d60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Nếu chưa cài đặt bộ tokenizer:\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def sentence_gleu(source: str, hypothesis: str, reference: str, max_n: int = 4) -> float:\n",
    "    source_tokens = nltk.word_tokenize(source)\n",
    "    hyp_tokens = nltk.word_tokenize(hypothesis)\n",
    "    ref_tokens = nltk.word_tokenize(reference)\n",
    "    \n",
    "    overlap = 0\n",
    "    total = 0\n",
    "\n",
    "    for n in range(1, max_n + 1):\n",
    "        hyp_ngrams = Counter(ngrams(hyp_tokens, n))\n",
    "        ref_ngrams = Counter(ngrams(ref_tokens, n))\n",
    "        src_ngrams = Counter(ngrams(source_tokens, n))\n",
    "\n",
    "        ref_diff = ref_ngrams - src_ngrams\n",
    "        hyp_diff = hyp_ngrams - src_ngrams\n",
    "\n",
    "        match = sum((hyp_diff & ref_diff).values())\n",
    "        total_hyp = sum(hyp_diff.values())\n",
    "\n",
    "        overlap += match\n",
    "        total += total_hyp\n",
    "\n",
    "    if total == 0:\n",
    "        return 1.0 if sum((ref_ngrams - src_ngrams).values()) == 0 else 0.0\n",
    "    return overlap / total\n",
    "\n",
    "def compute_corpus_gleu(sources, predictions, references, max_n=4):\n",
    "    \"\"\"\n",
    "    Tính GLEU trung bình trên toàn bộ tập dữ liệu.\n",
    "    - sources: list câu gốc (chưa sửa)\n",
    "    - predictions: list câu do model sửa\n",
    "    - references: list câu đúng\n",
    "    \"\"\"\n",
    "    assert len(sources) == len(predictions) == len(references), \"Danh sách không khớp độ dài\"\n",
    "\n",
    "    scores = [\n",
    "        sentence_gleu(src, pred, ref, max_n=max_n)\n",
    "        for src, pred, ref in zip(sources, predictions, references)\n",
    "    ]\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_corpus_gleu(error_sentences[:300], outputs_sentences[:], ref_sentences[:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "841b2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "93cf9b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (4.49.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc4a9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "def compute_bert_score(predictions, references, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Tính điểm BERTScore giữa các câu dự đoán và câu tham chiếu.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): danh sách các câu đầu ra từ mô hình.\n",
    "        references (list): danh sách các câu đúng thực tế.\n",
    "        lang (str): ngôn ngữ (mặc định là \"en\" cho tiếng Anh).\n",
    "        \n",
    "    Returns:\n",
    "        dict: Precision, Recall, F1 trung bình và danh sách chi tiết.\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(predictions, references, lang=lang)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": round(P.mean().item(), 4),\n",
    "        \"recall\": round(R.mean().item(), 4),\n",
    "        \"f1\": round(F1.mean().item(), 4),\n",
    "        \"f1_list\": [round(f.item(), 4) for f in F1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2727774",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result = compute_bert_score(outputs_sentences[:300], ref_sentences[:300])\n",
    "print(\"BERTScore F1:\", bert_result[\"f1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
