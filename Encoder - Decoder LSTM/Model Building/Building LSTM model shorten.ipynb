{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w5C1jt3DVKKZ"
      },
      "outputs": [],
      "source": [
        "import re #thư viện dùng xử lý chuỗi\n",
        "import unicodedata #thư viện dùng decode câu thành unicode8\n",
        "import spacy #thư viện giúp xử lý, trích xuất thông tin từ văn bản \n",
        "from tqdm import tqdm #thư viện dùng để tạo thanh triến trình (cho train model)\n",
        "import pandas as pd #thư viện dùng để tạo ma trận dữ liệu\n",
        "import numpy as np #thư viện dùng để xử lý mảng nhiều chiều\n",
        "import random #thư viện tạo số ngẫu nhiên\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator #vẽ biểu đồ wordcloud\n",
        "import matplotlib.pyplot as plt #vẽ biểu đồ phân tích data\n",
        "import seaborn as sns #vẽ biểu đồ phân tích data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn0X7EbblsRe"
      },
      "source": [
        "# GEC_Baseline_Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bEgnGgxgnt9x"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import fbeta_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"cleaned_data.csv\") #đọc lại file csv đã lưu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1398232, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data[:500000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UNJEoxF6n9c_"
      },
      "outputs": [],
      "source": [
        "def preprocess(t, add_start_token, add_end_token):\n",
        "  t = str(t)\n",
        "  if add_start_token == True and add_end_token == False: #nếu thêm start token và không thêm end token\n",
        "    t = '<start>'+' '+t\n",
        "  if add_start_token == False and add_end_token == True: #nếu không thêm start token và thêm end token\n",
        "    t = t+' '+'<end>'\n",
        "  if add_start_token == True and add_end_token == True: #nếu thêm cả start token và end token\n",
        "    t = '<start>'+' '+t+' '+'<end>'\n",
        "\n",
        "  t = re.sub(' +', ' ', t) #loại bỏ khoảng trắng thừa\n",
        "  return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "atv3504on-2A"
      },
      "outputs": [],
      "source": [
        "encoder_input = [preprocess(line, add_start_token= True, add_end_token=True) for line in data['error']]\n",
        "decoder_input = [preprocess(line, add_start_token= True, add_end_token=False) for line in data['correct']]\n",
        "decoder_output = [preprocess(line, add_start_token= False, add_end_token=True) for line in data['correct']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfzeTgzkoAOZ",
        "outputId": "108bec6f-ef32-4007-9b70-8e04ad77c33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> bitcoin is for , this morning , which coindesk says . <end>\n",
            "<start> bitcoin goes for , this morning , according to coindesk .\n",
            "bitcoin goes for , this morning , according to coindesk . <end>\n",
            "500000 500000 500000\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input[0]) # in ra dòng đầu tiên của encoder_input\n",
        "print(decoder_input[0]) \n",
        "print(decoder_output[0]) #output của ecoder output\n",
        "print(len(encoder_input), len(decoder_input), len(decoder_output)) #số lượng dòng trong 3 cột"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500000, 3)\n"
          ]
        }
      ],
      "source": [
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qKW_LVgoBpA",
        "outputId": "5f479c27-1b19-4b3b-b349-e4d619db34ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500000, 32)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(filters='', split=\" \")\n",
        "tokenizer.fit_on_texts(encoder_input)\n",
        "word_index = tokenizer.word_index #tạo index cho token\n",
        "\n",
        "max_length = max([ len(row.split(\" \")) for row in encoder_input ]) #tìm độ dài lớn nhất của câu trong encoder_input\n",
        "INPUT_ENCODER_LENGTH = max_length \n",
        "\n",
        "enc_input_encoded = tokenizer.texts_to_sequences(encoder_input) #tokenize các từ thành dạng index\n",
        "enc_input_padded= pad_sequences(enc_input_encoded, maxlen=INPUT_ENCODER_LENGTH, padding=\"post\") #đệm các câu để có cùng độ dài thành câu lớn nhất\n",
        "\n",
        "print(enc_input_padded.shape)\n",
        "#lưu tokenizer\n",
        "import pickle\n",
        "with open('in_tokenizer_fastext.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID của token <start>: 1\n",
            "ID của token <end>: 2\n"
          ]
        }
      ],
      "source": [
        "#kiểm tra id token start và end\n",
        "print(\"ID của token <start>:\", tokenizer.word_index['<start>'])\n",
        "print(\"ID của token <end>:\", tokenizer.word_index['<end>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "print(INPUT_ENCODER_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvfDFrYloEk2",
        "outputId": "da15ab12-005e-4d12-e28d-3fadc357efae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> bitcoin is for , this morning , which coindesk says . <end>\n",
            "[    1  3466    11    13     5    22   793     5    54 64899   493     3\n",
            "     2     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input[0]) #in ra câu đầu tiên của encoder_input\n",
        "print(enc_input_padded[0]) #sau khi được chuyển sang index và padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XIsXdyC6oGUI"
      },
      "outputs": [],
      "source": [
        "decoder_data = decoder_input.copy() #copy decoder_input sang decoder_data\n",
        "decoder_data.extend(decoder_output) #nối decoder_output vào cuối decoder_data\n",
        "\n",
        "out_tokenizer = Tokenizer(filters='', split=\" \") #tạo tokenizer cho decoder_data\n",
        "out_tokenizer.fit_on_texts(decoder_data) #fit dữ liệu vào tokenizer\n",
        "word_index = out_tokenizer.word_index #tạo index cho từ\n",
        "\n",
        "max_length = max([ len(row.split(\" \")) for row in decoder_input ]) #tìm độ dài lớn nhất của câu trong decoder_input\n",
        "INPUT_DECODER_LENGTH = max_length\n",
        "with open('out_tokenizer_fastext.pkl', 'wb') as f:\n",
        "    pickle.dump(out_tokenizer, f) #lưu tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID của token <start>: 4\n",
            "ID của token <end>: 5\n"
          ]
        }
      ],
      "source": [
        "print(\"ID của token <start>:\", out_tokenizer.word_index['<start>'])\n",
        "print(\"ID của token <end>:\", out_tokenizer.word_index['<end>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at64NfTgoIbM",
        "outputId": "2d21cada-58ca-4833-96ae-8f1bbd23d1c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500000, 32)\n"
          ]
        }
      ],
      "source": [
        "dec_input_encoded = out_tokenizer.texts_to_sequences(decoder_input) #chuyển các từ thành index\n",
        "dec_input_padded= pad_sequences(dec_input_encoded, maxlen=INPUT_DECODER_LENGTH, padding=\"post\", truncating = \"post\") #đệm các câu để có cùng độ dài thành câu lớn nhất\n",
        "\n",
        "print(dec_input_padded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvYjgUx9oJqQ",
        "outputId": "10955eac-54fc-47f8-f82e-472c71642f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> bitcoin goes for , this morning , according to coindesk .\n",
            "[    4  3332  1168    12     3    23   802     3   434     7 57414     1\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "print(decoder_input[0]) #in ra câu đầu tiên của decoder_input\n",
        "print(dec_input_padded[0]) #sau khi được chuyển sang index và padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgR-pGTgoLFC",
        "outputId": "bec58831-0776-4928-8978-abbb8339337b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500000, 32)\n"
          ]
        }
      ],
      "source": [
        "dec_output_encoded = out_tokenizer.texts_to_sequences(decoder_output) #chuyển các từ thành index\n",
        "dec_output_padded= pad_sequences(dec_output_encoded, maxlen=INPUT_DECODER_LENGTH, padding=\"post\", truncating = \"post\") #đệm các câu để có cùng độ dài thành câu lớn nhất\n",
        "\n",
        "print(dec_output_padded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYITIGocoMRj",
        "outputId": "f55e7266-2c5f-4d79-e77a-a10ecfbf612d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " . the effect of widespread dud targets two face up attack position monsters on the field . <end>\n",
            "[    1     2  1147     8  7715 57415  3795   106   708    51  1874   894\n",
            "  8241    15     2   513     1     5     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "print(decoder_output[1])\n",
        "print(dec_output_padded[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a5-rAoY0uwWe"
      },
      "outputs": [],
      "source": [
        "import io #thư viện dùng để xử lý file\n",
        "\n",
        "def load_vectors(fname): #hàm load vector từ file\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') #mở file\n",
        "    n, d = map(int, fin.readline().split()) #đọc số lượng từ và số chiều của từ\n",
        "    data = {} #tạo dictionary lưu từ và vector\n",
        "    for line in fin: #đọc từng dòng trong file\n",
        "        tokens = line.rstrip().split(' ') #tách từng từ và vector\n",
        "        data[tokens[0]] = np.asarray(tokens[1:]) #lưu từ và vector vào dictionary\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKYvOahooPb-",
        "outputId": "fe218cfb-47d8-4b28-fc20-45d9fd0bbe95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(310385, 300) (209334, 300)\n"
          ]
        }
      ],
      "source": [
        "in_embedding_matrix = np.load(r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Attachment\\in_embedding.npy\")\n",
        "out_embedding_matrix = np.load(r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Attachment\\out_embedding.npy\")\n",
        "print(in_embedding_matrix.shape, out_embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = inp_vocab_size #số lượng từ vựng\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_units = lstm_size\n",
        "        self.input_length = input_length\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(in_embedding_matrix), freeze=True,padding_idx=0)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.lstm_units,batch_first=True)\n",
        "    def forward(self,input_sequence,states):\n",
        "        input_embedding = self.embedding(input_sequence)\n",
        "        self.lstm_output, (self.state_h, self.state_c) = self.lstm(input_embedding, states)\n",
        "        return self.lstm_output, self.state_h, self.state_c\n",
        "    def initialize_states(self,batch_size):\n",
        "        lstm_state_h = torch.zeros((1,batch_size,self.lstm_units)).to(device)\n",
        "        lstm_state_c = torch.zeros((1,batch_size,self.lstm_units)).to(device)\n",
        "        return lstm_state_h, lstm_state_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dot_func(encoder_output,decoder_hidden_state):\n",
        "    decoder_hidden_state = torch.reshape(decoder_hidden_state,[decoder_hidden_state.shape[1],1,decoder_hidden_state.shape[2]])\n",
        "    dot_product = torch.matmul(encoder_output,decoder_hidden_state.transpose(1,2))\n",
        "    return dot_product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,scoring_function, att_units):\n",
        "        super(Attention,self).__init__()\n",
        "        self.scoring_function = scoring_function\n",
        "        self.att_units = att_units\n",
        "        self.timesteps = 0\n",
        "    def forward(self,decoder_hidden_state,encoder_output):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        encoder_output = encoder_output.to(device)\n",
        "        decoder_hidden_state = decoder_hidden_state.to(device)\n",
        "        if self.scoring_function == 'dot':\n",
        "            alpha = F.softmax(dot_func(encoder_output,decoder_hidden_state),dim = 1)\n",
        "            c_t = torch.sum(alpha *encoder_output,dim = 1)\n",
        "            return c_t,alpha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class One_Step_Decoder(nn.Module):\n",
        "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "        super(One_Step_Decoder,self).__init__()\n",
        "\n",
        "        self.tar_vocab_size = tar_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.dec_units = dec_units\n",
        "        self.score_fun = score_fun\n",
        "        self.att_units = att_units\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(out_embedding_matrix), freeze=True,padding_idx=0)\n",
        "        self.lstm = nn.LSTM(self.dec_units + self.embedding_dim,self.dec_units,batch_first=True)\n",
        "        self.dense = nn.Linear(self.dec_units,self.tar_vocab_size)\n",
        "    def forward(self,input_to_decoder,encoder_output,state_h,state_c):\n",
        "        input_embedding = self.embedding(input_to_decoder)\n",
        "        if self.score_fun == \"dot\":\n",
        "            attention = Attention(\"dot\",self.att_units)\n",
        "            context_vector,attention_weights = attention(state_h,encoder_output)\n",
        "        out = torch.cat([input_embedding,context_vector.unsqueeze(1)],dim = 2)\n",
        "        self.lstm_output, (self.state_h, self.state_c_) = self.lstm(out, (state_h,state_c))\n",
        "        result_out = self.dense(self.lstm_output)\n",
        "        return result_out.squeeze(1), self.state_h, self.state_c_, attention_weights, context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,out_vocab_size,embedding_dim,input_length,dec_units, score_fun,att_units):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.out_vocab_size = out_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.dec_units = dec_units\n",
        "        self.score_fun = score_fun\n",
        "        self.att_units = att_units\n",
        "        self.one_step_decoder = One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.input_length, self.dec_units ,self.score_fun ,self.att_units)\n",
        "    def forward(self,input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        input_to_decoder = input_to_decoder.to(device)\n",
        "        encoder_output = encoder_output.to(device)\n",
        "        decoder_hidden_state = decoder_hidden_state.to(device)\n",
        "        decoder_cell_state = decoder_cell_state.to(device)\n",
        "\n",
        "        batch_size, timesteps = input_to_decoder.shape\n",
        "        out_array = torch.zeros((batch_size, timesteps, self.one_step_decoder.tar_vocab_size)).to(device)\n",
        "\n",
        "        for timestep in range(timesteps):\n",
        "            output,decoder_hidden_state,decoder_cell_state,_,_ = self.one_step_decoder(input_to_decoder[:,timestep:timestep+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
        "            out_array[:,timestep,:] = output\n",
        "        return out_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class encoder_decoder(nn.Module):\n",
        "    def __init__(self,enc_units,dec_units,scoring_func,att_units):\n",
        "        super(encoder_decoder,self).__init__()\n",
        "        self.scoring_func = scoring_func\n",
        "        self.att_units = att_units\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.encoder = Encoder(INPUT_VOCAB_SIZE, embedding_size = 300, lstm_size= self.enc_units , input_length= INPUT_ENCODER_LENGTH)\n",
        "        self.decoder = Decoder(OUTPUT_VOCAB_SIZE, embedding_dim=300, input_length = None, dec_units= self.dec_units ,score_fun =self.scoring_func,att_units = self.att_units)\n",
        "    \n",
        "    def forward(self,data):\n",
        "        input,output = data[0],data[1]\n",
        "        states = self.encoder.initialize_states(input.shape[0])\n",
        "        encoder_output,encoder_final_state_h,encoder_final_state_c = self.encoder(input, states)\n",
        "        decoder_output = self.decoder(output,encoder_output, encoder_final_state_h, encoder_final_state_c)\n",
        "        return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8_dynsjoYnz",
        "outputId": "37a1f352-3b0d-4f14-9d3b-7e5f36aea453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "310385 32 209334 32\n"
          ]
        }
      ],
      "source": [
        "INPUT_VOCAB_SIZE = len(list(tokenizer.word_index)) +2 #kích thước từ vựng đầu vào +2 là do thêm start token và end token\n",
        "OUTPUT_VOCAB_SIZE = len(list(out_tokenizer.word_index)) +2 #tương tự như trên\n",
        "print(INPUT_VOCAB_SIZE, INPUT_ENCODER_LENGTH, OUTPUT_VOCAB_SIZE, INPUT_DECODER_LENGTH) #in ra kích thước tập từ vựng, đọ dài câu, kích thước tập từ vựng đầu ra, độ dài câu đầu ra, kích thước batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dec_input_padded' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_input_padded[idx],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_input_padded[idx],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_output_padded[idx]\n\u001b[1;32m---> 14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Seq2SeqDataset(enc_input_padded,\u001b[43mdec_input_padded\u001b[49m,dec_output_padded)\n\u001b[0;32m     15\u001b[0m test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m     16\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m test_size\n",
            "\u001b[1;31mNameError\u001b[0m: name 'dec_input_padded' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,enc_input_padded,dec_input_padded,dec_output_padded):\n",
        "        self.enc_input_padded = torch.tensor(enc_input_padded)\n",
        "        self.dec_input_padded = torch.tensor(dec_input_padded)\n",
        "        self.dec_output_padded = torch.tensor(dec_output_padded)\n",
        "    def __len__(self):\n",
        "        return len(self.enc_input_padded)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.enc_input_padded[idx],self.dec_input_padded[idx],self.dec_output_padded[idx]\n",
        "\n",
        "\n",
        "dataset = Seq2SeqDataset(enc_input_padded,dec_input_padded,dec_output_padded)\n",
        "test_size = int(len(dataset)*0.3)\n",
        "train_size = len(dataset) - test_size\n",
        "train_dataset,test_dataset = torch.utils.data.random_split(dataset,[train_size,test_size])\n",
        "\n",
        "#lưu training và test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(train_dataset, 'train_dataset.pt')\n",
        "torch.save(test_dataset, 'test_dataset.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18572\\4009569104.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_dataset = torch.load('train_dataset.pt')\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18572\\4009569104.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_dataset = torch.load('test_dataset.pt')\n"
          ]
        }
      ],
      "source": [
        "#load training và test dataset\n",
        "train_dataset = torch.load('train_dataset.pt')\n",
        "test_dataset = torch.load('test_dataset.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iAwTWx5ectH",
        "outputId": "ff43170a-2477-4da5-f24b-70c2ea9cc216"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = 32,shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size = 32,shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 32])\n",
            "torch.Size([32, 32])\n",
            "torch.Size([32, 32])\n",
            "Alo torch.Size([32, 32, 1])\n",
            "torch.Size([32, 256])\n",
            "torch.Size([32, 32, 300])\n",
            "torch.Size([32, 32, 256]) torch.Size([1, 32, 256]) torch.Size([1, 32, 256])\n",
            "torch.Size([32, 1, 256])\n",
            "torch.Size([32, 1, 556])\n"
          ]
        }
      ],
      "source": [
        "for a,b,c in train_loader:\n",
        "    print(a.shape)\n",
        "    print(b.shape)\n",
        "    print(c.shape)\n",
        "    break\n",
        "#thử cho encoder_input_padded đi qua lớp embedding\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "a,b,c = next(iter(train_loader))\n",
        "a = torch.randn(32, 32, 256, device=device)  # PHẢI có dòng này\n",
        "b = torch.randint(0, 1000, (32,1 ,256)).to(device).float()\n",
        "c= torch.matmul(a,b.transpose(1,2))\n",
        "print(\"Alo\", c.shape)\n",
        "alpha = F.softmax(c, dim=1)\n",
        "print(torch.sum(alpha * a, dim=1).shape)\n",
        "#thử lớp LSTM\n",
        "embedding = nn.Embedding.from_pretrained(torch.FloatTensor(in_embedding_matrix), freeze=True,padding_idx=0).to(device)\n",
        "a = torch.randint(0, 1000, (32,32)).to(device).long()\n",
        "print(embedding(a).shape)\n",
        "lstm = nn.LSTM(300,256,batch_first=True).to(device)\n",
        "lstm_output, (state_h, state_c) = lstm(embedding(a))\n",
        "print(lstm_output.shape, state_h.shape, state_c.shape)\n",
        "\n",
        "context_vecto = torch.randn(size = (32, 256)).to(device)\n",
        "context_vecto = context_vecto.unsqueeze(1)\n",
        "print(context_vecto.shape)\n",
        "input_embedding = torch.randn(size = (32, 1, 300)).to(device)\n",
        "print(torch.cat([input_embedding,context_vecto],dim =2).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18572\\2617672919.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\Downloads\\lang8model\\Model_NLP\\NLP\\len30_checkpoint.pt\")) #đã train được 5 epoch\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "encoder_decoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(310385, 300, padding_idx=0)\n",
              "    (lstm): LSTM(300, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (one_step_decoder): One_Step_Decoder(\n",
              "      (embedding): Embedding(209334, 300, padding_idx=0)\n",
              "      (lstm): LSTM(556, 256, batch_first=True)\n",
              "      (dense): Linear(in_features=256, out_features=209334, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = encoder_decoder(enc_units=256,dec_units=256,scoring_func=\"dot\",att_units=256)\n",
        "model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\Downloads\\lang8model\\Model_NLP\\NLP\\len30_checkpoint.pt\")) #đã train được 5 + 1 epoch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tham số của mô hình: 211119466\n",
            "Tham số huấn luyện: 55203766\n"
          ]
        }
      ],
      "source": [
        "#tổng tham số của mô hình\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "#tổng tham số huấn luyện\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Tham số của mô hình: {total_params}\")\n",
        "print(f\"Tham số huấn luyện: {trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6boftURjog_H"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, path='len30_checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss, model,epoch):\n",
        "        if val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            torch.save(model.state_dict(), self.path)  # Lưu model tốt nhất\n",
        "            print(f\"✅ Model từ epoch {epoch+1} đã được lưu tại {self.path}.\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"❌ Model từ epoch {epoch+1} không được lưu.\")\n",
        "            if self.counter >= self.patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Admin\\_netrc\n",
            "wandb: Currently logged in as: quoc-trieu-geckokidz (quoc-trieu-geckokidz-hust) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#đăng nhập vào wandb\n",
        "import wandb\n",
        "wandb.login(key = \"1a1e9d904fb11812f635b8c3f9a93ae09da4cd04\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Admin\\Downloads\\lang8model\\Model_NLP\\NLP\\wandb\\run-20250527_151553-47lppxjh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/quoc-trieu-geckokidz-hust/project_nlp_c4_fromepoch0/runs/47lppxjh' target=\"_blank\">first_run</a></strong> to <a href='https://wandb.ai/quoc-trieu-geckokidz-hust/project_nlp_c4_fromepoch0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/quoc-trieu-geckokidz-hust/project_nlp_c4_fromepoch0' target=\"_blank\">https://wandb.ai/quoc-trieu-geckokidz-hust/project_nlp_c4_fromepoch0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/quoc-trieu-geckokidz-hust/project_nlp_c4_fromepoch0/runs/47lppxjh' target=\"_blank\">https://wandb.ai/quoc-trieu-geckokidz-hust/project_nlp_c4_fromepoch0/runs/47lppxjh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/quoc-trieu-geckokidz-hust/project_nlp_c4_fromepoch0/runs/47lppxjh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x22cc7557080>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"project_nlp_c4_fromepoch0\", name=\"first_run\", config={\"epochs\": 10, \"lr\": 1e-3,\"time\":datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm  # Import tqdm để hiển thị tiến trình\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-3, patience=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        # Thanh tiến trình cho Training\n",
        "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=False)\n",
        "\n",
        "        for enc_input_padded, dec_input_padded, dec_output_padded in train_progress:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            enc_input_padded = enc_input_padded.to(device)\n",
        "            dec_input_padded = dec_input_padded.to(device)\n",
        "            dec_output_padded = dec_output_padded.to(device)\n",
        "\n",
        "            outputs = model([enc_input_padded, dec_input_padded])\n",
        "            outputs = outputs.view(-1, outputs.size(-1))  # (batch_size * seq_len, vocab_size)\n",
        "            targets = dec_output_padded.view(-1)  # (batch_size * seq_len)\n",
        "\n",
        "            loss = loss_function(outputs.float(), targets.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_progress.set_postfix(loss=loss.item())\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # ----- Validation -----\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for enc_input_padded, dec_input_padded, dec_output_padded in val_progress:\n",
        "                enc_input_padded = enc_input_padded.to(device)\n",
        "                dec_input_padded = dec_input_padded.to(device)\n",
        "                dec_output_padded = dec_output_padded.to(device)\n",
        "\n",
        "                outputs = model([enc_input_padded, dec_input_padded])\n",
        "                outputs = outputs.view(-1, outputs.size(-1))\n",
        "                targets = dec_output_padded.view(-1)\n",
        "\n",
        "                loss = loss_function(outputs.float(), targets.long())\n",
        "                val_loss += loss.item()\n",
        "                val_progress.set_postfix(val_loss=loss.item())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Log thông số lên WandB\n",
        "        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"val_loss\": val_loss, \"learning_rate\": optimizer.param_groups[0][\"lr\"]})\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Lưu mô hình nếu có cải thiện\n",
        "        if early_stopping(val_loss, model, epoch):\n",
        "            wandb.run.summary[\"best_val_loss\"] = val_loss\n",
        "            torch.save(model.state_dict(), \"len30_best_model.pth\")\n",
        "            wandb.save(\"best_model.pth\")\n",
        "            break\n",
        "\n",
        "    # Kết thúc phiên làm việc với WandB\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Train Loss: 1.7783 - Val Loss: 2.0109\n",
            "✅ Model từ epoch 1 đã được lưu tại len30_checkpoint.pt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 - Train Loss: 1.5960 - Val Loss: 1.9909\n",
            "✅ Model từ epoch 2 đã được lưu tại len30_checkpoint.pt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 - Train Loss: 1.4974 - Val Loss: 1.9895\n",
            "✅ Model từ epoch 3 đã được lưu tại len30_checkpoint.pt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 - Train Loss: 1.4426 - Val Loss: 1.9812\n",
            "✅ Model từ epoch 4 đã được lưu tại len30_checkpoint.pt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 - Train Loss: 1.4138 - Val Loss: 1.9888\n",
            "❌ Model từ epoch 5 không được lưu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                         \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[38], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr, patience)\u001b[0m\n\u001b[0;32m     30\u001b[0m targets \u001b[38;5;241m=\u001b[39m dec_output_padded\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size * seq_len)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs\u001b[38;5;241m.\u001b[39mfloat(), targets\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#train model\n",
        "train_model(model, train_loader, test_loader, num_epochs=10, lr=1e-3, patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),\"last_model_28_05.pth\") #lưu model cuối cùng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22052\\2455360464.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\Downloads\\NLP\\best_model.pth\"))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "encoder_decoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(52381, 300, padding_idx=0)\n",
              "    (lstm): LSTM(300, 512, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (one_step_decoder): One_Step_Decoder(\n",
              "      (embedding): Embedding(41061, 300, padding_idx=0)\n",
              "      (lstm): LSTM(812, 512, batch_first=True)\n",
              "      (dense): Linear(in_features=512, out_features=41061, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\Downloads\\NLP\\best_model.pth\"))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ihNsbnC6pTDX",
        "outputId": "0374ce5b-319c-41e1-a365-0282548f7935"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "encoder_decoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(52381, 300, padding_idx=0)\n",
              "    (lstm): LSTM(300, 512, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (one_step_decoder): One_Step_Decoder(\n",
              "      (embedding): Embedding(41061, 300, padding_idx=0)\n",
              "      (lstm): LSTM(812, 512, batch_first=True)\n",
              "      (dense): Linear(in_features=512, out_features=41061, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
