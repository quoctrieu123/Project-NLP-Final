{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e40f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu gốc: he are the best football player.\n",
      "Câu đã sửa: he is the best football player .\n"
     ]
    }
   ],
   "source": [
    "from create_model import create_model\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "from data_preprocess import input_processor, preprocess, predict_sentence\n",
    "link_to_in_tokenizer = r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Attachment\\tokenizer.pickle\"\n",
    "link_to_out_tokenizer = r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Attachment\\out_tokenizer.pickle\"\n",
    "link_to_in_embedding = r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Attachment\\in_embedding.npy\" #chủ động thay đổi đường dẫn tới file\n",
    "link_to_out_embedding = r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Attachment\\out_embedding.npy\"#chủ động thay đổi đường dẫn tới file\n",
    "model = create_model(link_to_in_embedding,link_to_out_embedding) #load model đã train\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Saved\\best_model.pth\")) #chủ động thay đổi đường dẫn tới file\n",
    "input_sentence = \"he are the best football player.\" #câu cần sửa\n",
    "output_sentence = predict_sentence(model,input_sentence, link_to_in_tokenizer, link_to_out_tokenizer) #dự đoán câu\n",
    "print(\"Câu gốc:\", input_sentence)\n",
    "print(\"Câu đã sửa:\", output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79313af",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "test_sentences = []\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\Model_NLP\\NLP\\Model Test\\lang_8_cleaned.csv\")\n",
    "data = data[:300]\n",
    "data.rename(columns={0: \"error\", 1: \"correct\"}, inplace=True)\n",
    "data = data[data['error'].str.split().str.len() <= 32]\n",
    "error_sentences = [str(sentence) for sentence in data[\"error\"]]\n",
    "ref_sentences = [str(sentence) for sentence in data[\"correct\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2e54a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_sentences = []\n",
    "for sentence in error_sentences[:300]:\n",
    "    decoded = predict_sentence(model, sentence, link_to_in_tokenizer, link_to_out_tokenizer)\n",
    "    outputs_sentences.append(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a9c2b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a55c009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "def decontracted(phrase): #hàm chuyển từ viết tắt thành từ đầy đủ\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"gon na\", \" going to\", phrase)\n",
    "    phrase = re.sub(r\"wan na\", \" want to\", phrase)\n",
    "    phrase = re.sub(r\"gonna\", \" going to\", phrase)\n",
    "    phrase = re.sub(r\"wanna\", \" want to\", phrase)\n",
    "\n",
    "\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def clean_text(t): #hàm bỏ dấu và ký tự unicode\n",
    "\n",
    "  t = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('ascii') \n",
    "  t = decontracted(t)\n",
    "\n",
    "  t = re.sub(r'x D', '', t)\n",
    "  t = re.sub(r': D', '', t)\n",
    "  t = re.sub(r': P', '', t)\n",
    "\n",
    "  t = re.sub(r'xD', '', t)\n",
    "  t = re.sub(r':D', '', t)\n",
    "  t = re.sub(r':P', '', t)\n",
    "\n",
    "  if '(' in t and ')' in t: #loại bỏ nội dung trong dấu ngoặc đơn => Hello (world) => Hello\n",
    "    try:\n",
    "      t = re.sub(t.split(\"(\")[-1].split(\")\")[0], '', t)\n",
    "    except:\n",
    "      pass\n",
    "    #t = re.sub(\"(\", '', t)\n",
    "    #t = re.sub(\")\", '', t)\n",
    "\n",
    "  t = re.sub(r'[^A-Za-z;!?.,\\-\\' ]+', ' ', t) #loại bỏ các emoji, số, ký tự đặc biệt\n",
    "\n",
    "#chuẩn hóa dấu câu, thêm khoảng trống trước dâu câu. \n",
    "  t = re.sub(r'\\.+',r' .',t)\n",
    "  t = re.sub(r'\\;+',r' , ',t)\n",
    "  t = re.sub(r'!+',r' !',t )\n",
    "  t = re.sub(r'\\?+',r' ?',t )\n",
    "  t = re.sub(r'\\-+',r' - ',t )\n",
    "  t = re.sub(r'\\,+',r' , ',t )\n",
    "  t = re.sub(r'\\'+',r\" ' \",t)\n",
    "  t = re.sub(' +', ' ', t)\n",
    "  t = t.lower()\n",
    "\n",
    "  return t #trả về văn bản được làm sạch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e50d3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sentences = [clean_text(sentence) for sentence in ref_sentences]\n",
    "error_sentences = [clean_text(sentence) for sentence in error_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94365f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sentences: well , finally , my mother took me to the customs window , and she gave me a passport to the man . . .\n",
      "Ref sentences: ifinally my mother took me to the customs window , and gave a passport to the man . . .\n",
      "Error sentences: well , finally , my mother took me to the customs window , she gave a passport to man . . .\n"
     ]
    }
   ],
   "source": [
    "i = 11\n",
    "print(f\"Output sentences: {outputs_sentences[i]}\")\n",
    "print(f\"Ref sentences: {ref_sentences[i]}\")\n",
    "print(f\"Error sentences: {error_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "532bcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Nếu chưa cài đặt bộ tokenizer:\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def sentence_gleu(source: str, hypothesis: str, reference: str, max_n: int = 4) -> float:\n",
    "    source_tokens = nltk.word_tokenize(source)\n",
    "    hyp_tokens = nltk.word_tokenize(hypothesis)\n",
    "    ref_tokens = nltk.word_tokenize(reference)\n",
    "    \n",
    "    overlap = 0\n",
    "    total = 0\n",
    "\n",
    "    for n in range(1, max_n + 1):\n",
    "        hyp_ngrams = Counter(ngrams(hyp_tokens, n))\n",
    "        ref_ngrams = Counter(ngrams(ref_tokens, n))\n",
    "        src_ngrams = Counter(ngrams(source_tokens, n))\n",
    "\n",
    "        ref_diff = ref_ngrams - src_ngrams\n",
    "        hyp_diff = hyp_ngrams - src_ngrams\n",
    "\n",
    "        match = sum((hyp_diff & ref_diff).values())\n",
    "        total_hyp = sum(hyp_diff.values())\n",
    "\n",
    "        overlap += match\n",
    "        total += total_hyp\n",
    "\n",
    "    if total == 0:\n",
    "        return 1.0 if sum((ref_ngrams - src_ngrams).values()) == 0 else 0.0\n",
    "    return overlap / total\n",
    "\n",
    "def compute_corpus_gleu(sources, predictions, references, max_n=4):\n",
    "    \"\"\"\n",
    "    Tính GLEU trung bình trên toàn bộ tập dữ liệu.\n",
    "    - sources: list câu gốc (chưa sửa)\n",
    "    - predictions: list câu do model sửa\n",
    "    - references: list câu đúng\n",
    "    \"\"\"\n",
    "    assert len(sources) == len(predictions) == len(references), \"Danh sách không khớp độ dài\"\n",
    "\n",
    "    scores = [\n",
    "        sentence_gleu(src, pred, ref, max_n=max_n)\n",
    "        for src, pred, ref in zip(sources, predictions, references)\n",
    "    ]\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da351612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLEU Score of the LSTM Model: 0.2876030857422058\n"
     ]
    }
   ],
   "source": [
    "print(f\"GLEU Score of the LSTM Model: {compute_corpus_gleu(error_sentences[:300], outputs_sentences[:], ref_sentences[:300])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e834d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-score in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (4.49.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (3.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5d3b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "def compute_bert_score(predictions, references, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Tính điểm BERTScore giữa các câu dự đoán và câu tham chiếu.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): danh sách các câu đầu ra từ mô hình.\n",
    "        references (list): danh sách các câu đúng thực tế.\n",
    "        lang (str): ngôn ngữ (mặc định là \"en\" cho tiếng Anh).\n",
    "        \n",
    "    Returns:\n",
    "        dict: Precision, Recall, F1 trung bình và danh sách chi tiết.\n",
    "    \"\"\"\n",
    "    P, R, F1 = score(predictions, references, lang=lang)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": round(P.mean().item(), 4),\n",
    "        \"recall\": round(R.mean().item(), 4),\n",
    "        \"f1\": round(F1.mean().item(), 4),\n",
    "        \"f1_list\": [round(f.item(), 4) for f in F1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b291fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.9653\n"
     ]
    }
   ],
   "source": [
    "bert_result = compute_bert_score(outputs_sentences[:300], ref_sentences[:300])\n",
    "print(\"BERTScore F1:\", bert_result[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb7cbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viết câu sai vào file source.txt\n",
    "with open(r\"C:\\Users\\Admin\\Downloads\\errant_eva\\source_lang_8.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in error_sentences[:300]:\n",
    "        f.write(sentence + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a10b3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Admin\\Downloads\\errant_eva\\reference_lang_8.txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "    for sentence in ref_sentences[:300]:\n",
    "        f.write(sentence + \"\\n\")\n",
    "\n",
    "with open(r\"C:\\Users\\Admin\\Downloads\\errant_eva\\hypothesis_lang_8.txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "    for sentence in outputs_sentences[:300]:\n",
    "        f.write(sentence + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
